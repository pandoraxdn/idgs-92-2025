from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

custom_responses = {
    "Â¿quiÃ©n eres?": "Soy un asistente conversacional basado en inteligencia artificial.",
    "Â¿cÃ³mo te llamas?": "Puedes llamarme PandoraIA.",
    "Â¿cual es tu nombre?": "Puedes llamarme PandoraIA.",
    "Â¿quÃ© puedes hacer?": "Puedo conversar contigo sobre muchos temas. Â¡PruÃ©bame!",
    "hola": "Â¡Hola! Â¿En quÃ© te puedo ayudar hoy?",
    "adiÃ³s": "Â¡Hasta pronto! ðŸ˜Š",
    "gracias": "Â¡De nada! Estoy para ayudarte."
}

def check_custom_response(user_input):
    user_input_lower = user_input.lower()
    for key in custom_responses:
        if key in user_input_lower:
            return custom_responses[key]
    return None

print("Cargando modelo...")
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-small")
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-small")
print("Modelo cargado.")

chat_history_ids = None

while True:
    user_input = input("TÃº: ")

    if user_input.lower() in ['salir', 'exit', 'quit']:
        print("PandoraIA: Â¡Hasta luego!")
        break

    custom_reply = check_custom_response(user_input)
    if custom_reply:
        print(f"PandoraIA: {custom_reply}")
        continue

    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')

    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids

    chat_history_ids = model.generate(
        bot_input_ids,
        max_length=1000,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50,
        top_p=0.95
    )

    bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)
    print(f"Bot: {bot_response}")

